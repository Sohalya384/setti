{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sohalya384/setti/blob/main/Ensemble%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "pcRIUiQdoQvL"
      },
      "source": [
        "## Loan Prediction Problem\n",
        "\n",
        "In this scenario we are trying generate a model to predict the loan status of a preson based on various parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA4r9aJioQvO"
      },
      "outputs": [],
      "source": [
        "#importing important packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C6z7IwBoQvP"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(r'C:\\Users\\J554696\\Desktop\\Ensemble\\train_clean_data.csv',index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHlvgj0GoQvQ"
      },
      "outputs": [],
      "source": [
        "df.dropna(axis=0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akie-95woQvQ",
        "outputId": "5df86f9f-e26c-4dd0-efeb-54759d0f5e50"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Loan_ID</th>\n",
              "      <th>Dependents</th>\n",
              "      <th>ApplicantIncome</th>\n",
              "      <th>CoapplicantIncome</th>\n",
              "      <th>LoanAmount</th>\n",
              "      <th>Loan_Amount_Term</th>\n",
              "      <th>Credit_History</th>\n",
              "      <th>gender_Male</th>\n",
              "      <th>married_Yes</th>\n",
              "      <th>education_Not Graduate</th>\n",
              "      <th>property_area_Semiurban</th>\n",
              "      <th>property_area_Urban</th>\n",
              "      <th>self_employed_Yes</th>\n",
              "      <th>Loan_status_Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LP001003</td>\n",
              "      <td>1</td>\n",
              "      <td>4583</td>\n",
              "      <td>1508.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>360.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LP001005</td>\n",
              "      <td>0</td>\n",
              "      <td>3000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>360.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LP001006</td>\n",
              "      <td>0</td>\n",
              "      <td>2583</td>\n",
              "      <td>2358.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>360.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LP001008</td>\n",
              "      <td>0</td>\n",
              "      <td>6000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>360.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LP001011</td>\n",
              "      <td>2</td>\n",
              "      <td>5417</td>\n",
              "      <td>4196.0</td>\n",
              "      <td>267.0</td>\n",
              "      <td>360.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Loan_ID Dependents  ApplicantIncome  CoapplicantIncome  LoanAmount  \\\n",
              "1  LP001003          1             4583             1508.0       128.0   \n",
              "2  LP001005          0             3000                0.0        66.0   \n",
              "3  LP001006          0             2583             2358.0       120.0   \n",
              "4  LP001008          0             6000                0.0       141.0   \n",
              "5  LP001011          2             5417             4196.0       267.0   \n",
              "\n",
              "   Loan_Amount_Term  Credit_History  gender_Male  married_Yes  \\\n",
              "1             360.0             1.0            1            1   \n",
              "2             360.0             1.0            1            1   \n",
              "3             360.0             1.0            1            1   \n",
              "4             360.0             1.0            1            0   \n",
              "5             360.0             1.0            1            1   \n",
              "\n",
              "   education_Not Graduate  property_area_Semiurban  property_area_Urban  \\\n",
              "1                       0                        0                    0   \n",
              "2                       0                        0                    1   \n",
              "3                       1                        0                    1   \n",
              "4                       0                        0                    1   \n",
              "5                       0                        0                    1   \n",
              "\n",
              "   self_employed_Yes  Loan_status_Y  \n",
              "1                  0              0  \n",
              "2                  1              1  \n",
              "3                  0              1  \n",
              "4                  0              1  \n",
              "5                  1              1  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LlpBFzmoQvR",
        "outputId": "9622b842-0f20-4e2c-ec62-638ee033b032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 517 entries, 1 to 613\n",
            "Data columns (total 14 columns):\n",
            "Loan_ID                    517 non-null object\n",
            "Dependents                 517 non-null object\n",
            "ApplicantIncome            517 non-null int64\n",
            "CoapplicantIncome          517 non-null float64\n",
            "LoanAmount                 517 non-null float64\n",
            "Loan_Amount_Term           517 non-null float64\n",
            "Credit_History             517 non-null float64\n",
            "gender_Male                517 non-null int64\n",
            "married_Yes                517 non-null int64\n",
            "education_Not Graduate     517 non-null int64\n",
            "property_area_Semiurban    517 non-null int64\n",
            "property_area_Urban        517 non-null int64\n",
            "self_employed_Yes          517 non-null int64\n",
            "Loan_status_Y              517 non-null int64\n",
            "dtypes: float64(4), int64(8), object(2)\n",
            "memory usage: 60.6+ KB\n"
          ]
        }
      ],
      "source": [
        "#df.rename({'Yes': 'Self_Employed'}, axis=1)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6rFhrbKoQvS"
      },
      "source": [
        "Similarly, fill values for all the columns. EDA, missing values and outlier treatment has been skipped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqKVk3vjoQvS"
      },
      "outputs": [],
      "source": [
        "x = df.drop(['Loan_status_Y','Loan_ID','Dependents'],axis=1)\n",
        "y = df['Loan_status_Y']\n",
        "\n",
        "#split dataset into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train , x_test , y_train , y_test = train_test_split(x , y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbE5QYyOoQvS"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_E9iSCFoQvS",
        "outputId": "09ba3991-e5a6-4564-db87-3edb47b76e6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
              "            oob_score=False, random_state=None, verbose=0,\n",
              "            warm_start=False)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "rfc.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKKfuDSMoQvT"
      },
      "outputs": [],
      "source": [
        "rfc_pred = rfc.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPh9lewAoQvT",
        "outputId": "87477589-0acf-4c81-9c8d-73039d3878cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 20  19]\n",
            " [  9 108]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report , confusion_matrix\n",
        "\n",
        "print(confusion_matrix(y_test,rfc_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my65w2S2oQvT",
        "outputId": "bef37ee9-04ff-42d8-c4e8-4238c189fe51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.69      0.51      0.59        39\n",
            "          1       0.85      0.92      0.89       117\n",
            "\n",
            "avg / total       0.81      0.82      0.81       156\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test,rfc_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGx8vnxHoQvU"
      },
      "source": [
        "## Bagging meta-estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTFLk-ThoQvU"
      },
      "source": [
        "Bagging meta-estimator is an ensembling algorithm that can be used for both **classification (BaggingClassifier)** and **regression (BaggingRegressor)** problems. It follows the typical bagging technique to make predictions. Following are the steps for the bagging meta-estimator algorithm:\n",
        "\n",
        "1. Random subsets are created from the original dataset (Bootstrapping).\n",
        "2. The subset of the dataset includes all features.\n",
        "3. A user-specified base estimator is fitted on each of these smaller sets.\n",
        "4. Predictions from each model are combined to get the final result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekEsBHTmoQvU"
      },
      "source": [
        "Parameters used in the  algorithms:\n",
        "\n",
        "**base_estimator**:\n",
        "It defines the base estimator to fit on random subsets of the dataset.\n",
        "When nothing is specified, the base estimator is a decision tree.\n",
        "\n",
        "**n_estimators**:\n",
        "It is the number of base estimators to be created.\n",
        "The number of estimators should be carefully tuned as a large number would take a very long time to run, while a very small number might not provide the best results.\n",
        "\n",
        "**max_samples**:\n",
        "This parameter controls the size of the subsets.\n",
        "It is the maximum number of samples to train each base estimator.\n",
        "\n",
        "**max_features**:\n",
        "Controls the number of features to draw from the whole dataset.\n",
        "It defines the maximum number of features required to train each base estimator.\n",
        "\n",
        "**n_jobs**:\n",
        "The number of jobs to run in parallel.\n",
        "Set this value equal to the cores in your system.\n",
        "If -1, the number of jobs is set to the number of cores.\n",
        "\n",
        "**random_state**:\n",
        "It specifies the method of random split. When random state value is same for two models, the random selection is same for both models.\n",
        "This parameter is useful when you want to compare different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDfCNBE2oQvU",
        "outputId": "8a1422ec-199c-428b-8e98-5498f7e33d7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BaggingClassifier(base_estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
              "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
              "          verbose=0, warm_start=False),\n",
              "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
              "         max_samples=1.0, n_estimators=100, n_jobs=1, oob_score=False,\n",
              "         random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "model = BaggingClassifier(LogisticRegression(),n_estimators=100)\n",
        "\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkg4N0u3oQvV",
        "outputId": "ac59d6eb-3746-4bd0-9d7a-040868a2ebd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8589743589743589"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.score(x_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzU4asbroQvV"
      },
      "source": [
        " Sample code for regression problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dyd3gooZoQvV",
        "outputId": "e22312ec-dec8-43e2-ae44-5d502a8e005a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.17811965811965813"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "model = BaggingRegressor(tree.DecisionTreeRegressor(random_state=1))\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_imP7_ekoQvV"
      },
      "source": [
        "# AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nStjY-FeoQvV"
      },
      "source": [
        "Adaptive boosting or AdaBoost is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.\n",
        "\n",
        "Below are the steps for performing the AdaBoost algorithm:\n",
        "\n",
        "1. Initially, all observations in the dataset are given equal weights.\n",
        "2. A model is built on a subset of data.\n",
        "3. Using this model, predictions are made on the whole dataset.\n",
        "4. Errors are calculated by comparing the predictions and actual values.\n",
        "5. While creating the next model, higher weights are given to the data points which were predicted incorrectly.\n",
        "6. Weights can be determined using the error value. For instance, higher the error more is the weight assigned to the observation.\n",
        "7. This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoBMR33GoQvW"
      },
      "source": [
        "### Ada Boost Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDVU1xKVoQvW",
        "outputId": "8e68467f-646a-4f33-fedd-81613e71cf9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8269230769230769"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "model = AdaBoostClassifier(random_state=1)\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMpiLRL7oQvW"
      },
      "source": [
        "### Ada Boost Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQtM9ayJoQvW",
        "outputId": "093e7e03-947b-4c9c-cc14-94d672482a9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.13244777456315393"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "model = AdaBoostRegressor()\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz3xjxcBoQvW"
      },
      "source": [
        "# Gradient Boosting (GBM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOU8mgGwoQvW"
      },
      "source": [
        "Gradient Boosting or GBM is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner. Regression trees used as a base learner, each subsequent tree in series is built on the errors calculated by the previous tree.\n",
        "\n",
        "We will use a simple example to understand the GBM algorithm. We have to predict the age of a group of people using the below data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XImZigxwoQvX"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSkx36vXoQvX"
      },
      "source": [
        "1. The mean age is assumed to be the predicted value for all observations in the dataset.\n",
        "2. The errors are calculated using this mean prediction and actual values of age."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dackNAR5oQvX"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyYeN5PioQvX"
      },
      "source": [
        "3.A tree model is created using the errors calculated above as target variable. Our objective is to find the best split to minimize the error.\n",
        "\n",
        "4.The predictions by this model are combined with the predictions 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkmQrV_DoQvX"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcfnlqdZoQvX"
      },
      "source": [
        "5.This value calculated above is the new prediction.\n",
        "\n",
        "6.New errors are calculated using this predicted value and actual value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiotjjezoQvX"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilcpEO2IoQvY"
      },
      "source": [
        "7.Steps 2 to 6 are repeated till the maximum number of iterations is reached (or error function does not change)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm_W4bskoQvY",
        "outputId": "11ccdb31-e698-4aa8-a16d-e00409d8071d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8269230769230769"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "model= GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnV2KFPuoQvY"
      },
      "source": [
        "### Gradient Boost regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geCkuRR_oQvY",
        "outputId": "6e144c6d-ee03-4f7d-e693-b219b6db25a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.1841870595920725"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "model= GradientBoostingRegressor()\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqLb7kq_oQvg"
      },
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbUs3uCNoQvg"
      },
      "source": [
        "XGBoost (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as **‘regularized boosting‘** technique.\n",
        "\n",
        "Let us see how XGBoost is comparatively better than other techniques:\n",
        "\n",
        "1. **Regularization**:\n",
        "Standard GBM implementation has no regularisation like XGBoost.\n",
        "Thus XGBoost also helps to reduce overfitting.\n",
        "2. **Parallel Processing**:\n",
        "XGBoost implements parallel processing and is faster than GBM .\n",
        "XGBoost also supports implementation on Hadoop.\n",
        "3. **High Flexibility**:\n",
        "XGBoost allows users to define custom optimization objectives and evaluation criteria adding a whole new dimension to the model.\n",
        "4. **Handling Missing Values**:\n",
        "XGBoost has an in-built routine to handle missing values.\n",
        "5. **Tree Pruning**:\n",
        "XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits beyond which there is no positive gain.\n",
        "6. **Built-in Cross-Validation**:\n",
        "XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzrM7ZYDoQvg"
      },
      "source": [
        "Since XGBoost takes care of the missing values itself, you do not have to impute the missing values. You can skip the step for missing value imputation from the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tMQp_4RoQvh"
      },
      "source": [
        "### Xg Boost Classification problem\n",
        "\n",
        "pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1WmXUV3JoQvh"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
        "#model.fit(x_train, y_train)\n",
        "#model.score(x_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcUwUk9hoQvh"
      },
      "source": [
        "### XG boost Regression problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwNCfM7UoQvh",
        "outputId": "6d1ae8b2-0473-4055-a1ba-e0866dc35c86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.19129385697867718"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "model=xgb.XGBRegressor()\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLRJ3EuJoQvi",
        "outputId": "57d257dd-661b-4b9b-895c-8321cafcdfc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([], shape=(0, 1), dtype=float64)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.empty((0,1),float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z4cZ2XFoQvi"
      },
      "source": [
        "## STACKING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36rzx3UboQvj"
      },
      "source": [
        "We first define a function to make predictions on n-folds of train and test dataset. This function returns the predictions for train and test for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "07m-gDt6oQvj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def Stacking(model,train,y,test,n_fold):\n",
        "    '''\n",
        "    n_fold=10,\n",
        "    train=x_train,\n",
        "    test=x_test,\n",
        "    y=y_train\n",
        "\n",
        "    StratifiedKFold: used for k folds cross varidation.\n",
        "    Stacking function is returning 2D numpy arrays\n",
        "\n",
        "    '''\n",
        "    folds=StratifiedKFold(n_splits=n_fold,random_state=1)\n",
        "\n",
        "\n",
        "    test_pred=np.empty((0,1),float)\n",
        "    train_pred=np.empty((0,1),float)\n",
        "\n",
        "\n",
        "    for train_indices,val_indices in folds.split(train,y.values):\n",
        "\n",
        "        x_train,x_val=train.iloc[train_indices],train.iloc[val_indices]\n",
        "        y_train,y_val=y.iloc[train_indices],y.iloc[val_indices]\n",
        "\n",
        "        model.fit(X=x_train,y=y_train)\n",
        "        train_pred=np.append(train_pred,model.predict(x_val))\n",
        "\n",
        "    test_pred=np.append(test_pred,model.predict(test))\n",
        "\n",
        "    return test_pred.reshape(-1,1),train_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGmrTAdxoQvk"
      },
      "source": [
        "Now we’ll create two base models – decision tree and knn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xtE4O7wFoQvk"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model1 = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "test_pred1 ,train_pred1=Stacking(model=model1,n_fold=10, train=x_train,test=x_test,y=y_train)\n",
        "\n",
        "train_pred1=pd.DataFrame(train_pred1)\n",
        "test_pred1=pd.DataFrame(test_pred1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tYnXslIFoQvk"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model2 = KNeighborsClassifier()\n",
        "\n",
        "test_pred2 ,train_pred2=Stacking(model=model2,n_fold=10,train=x_train,test=x_test,y=y_train)\n",
        "\n",
        "train_pred2=pd.DataFrame(train_pred2)\n",
        "test_pred2=pd.DataFrame(test_pred2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7HhtRJ8oQvl"
      },
      "source": [
        "Create a third model, logistic regression, on the predictions of the decision tree and knn models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3m9MiVsoQvl",
        "outputId": "f7038b47-69e4-42a7-c7ea-320a544999d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6923076923076923"
            ]
          },
          "execution_count": 191,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.concat([train_pred1, train_pred2], axis=1)\n",
        "df_test = pd.concat([test_pred1, test_pred2], axis=1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(random_state=1)\n",
        "model.fit(df,y_train)\n",
        "model.score(df_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--m1jE-SoQvl"
      },
      "source": [
        "In order to simplify the above explanation, the stacking model we have created has only two levels. The decision tree and knn models are built at level zero, while a logistic regression model is built at level one."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}